{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Huxwell/ColabNNs/blob/main/Eggs_EfficientDet_train_tf_convert_tflite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcdbc9bfaIDE"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanhlvg/tflite_raspberry_pi/blob/main/object_detection/Train_custom_model_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf2if_fGDaWc"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified by Filip Drapejkowski, https://www.youtube.com/watch?v=mNjXEybFn98&ab_channel=TensorFlow video I've used to find it."
      ],
      "metadata": {
        "id": "v4yy6Pl3bi_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn_WKsEci3Em",
        "outputId": "827ff398-8fbe-4c09-d242-12657a58b911"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/OVO-TECH/eggs_pascal_voc.zip ."
      ],
      "metadata": {
        "id": "R4jEMNhvjDLC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/eggs_pascal_voc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n8wlblTjwAn",
        "outputId": "aa1424be-1d12-4a84-cec7-627e978ce703"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/eggs_pascal_voc': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir eggs_pascal_voc\n",
        "!unzip -q /content/eggs_pascal_voc.zip -d eggs_pascal_voc"
      ],
      "metadata": {
        "id": "3kO2pdT8jKbq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "jrmj83afDJrv"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpJEzDG6DK2Q"
      },
      "source": [
        "# Train a custom object detection model with TensorFlow Lite Model Maker\n",
        "\n",
        "In this colab notebook, you'll learn how to use the [TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/guide/model_maker) to train a custom object detection model to detect Android figurines and how to put the model on a Raspberry Pi.\n",
        "\n",
        "The Model Maker library uses *transfer learning* to simplify the process of training a TensorFlow Lite model using a custom dataset. Retraining a TensorFlow Lite model with your own custom dataset reduces the amount of training data required and will shorten the training time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRYjtwRZGBOI"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "### Install the required packages\n",
        "Start by installing the required packages, including the Model Maker package from the [GitHub repo](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) and the pycocotools library you'll use for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35BJmtVpAP_n",
        "outputId": "44b142df-39ec-42de-9197-b10a57987ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.3/577.3 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 KB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.6/128.6 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 KB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 KB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.4/222.4 KB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q tflite-model-maker\n",
        "!pip install -q tflite-support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prQ86DdtD317"
      },
      "source": [
        "Import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l4QQTXHHATDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a10b748-25d7-4fdd-df04-b5c219975890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.4 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat, QuantizationConfig\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "from tflite_support import metadata\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g6aQvXsD78P"
      },
      "source": [
        "### Prepare the dataset\n",
        "\n",
        "This dataset contains about 70 images of 2 type of Android figurines: an Android and an Android pig. This is an example image from the dataset.\n",
        "\n",
        "![android_figurine_sample.jpg](https://storage.googleapis.com/download.tensorflow.org/example_images/android_figurine_sample.jpg)\n",
        "\n",
        "We start with downloading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AGg7D4JAV62",
        "outputId": "52151677-93c1-4b46-fe35-c7ebc9bece6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-26 11:00:57--  https://storage.googleapis.com/download.tensorflow.org/data/android_figurine.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.97.128, 173.194.214.128, 142.251.107.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.97.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14333895 (14M) [application/zip]\n",
            "Saving to: ‘android_figurine.zip’\n",
            "\n",
            "android_figurine.zi 100%[===================>]  13.67M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-02-26 11:00:57 (99.4 MB/s) - ‘android_figurine.zip’ saved [14333895/14333895]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/download.tensorflow.org/data/android_figurine.zip\n",
        "!unzip -q android_figurine.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/eggs_pascal_voc/labels_and_images"
      ],
      "metadata": {
        "id": "Fjsmd9NLkHx5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install imagemagick"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga-bY9gef4Qr",
        "outputId": "78d16798-1618-4518-f2b6-601136d81723"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 ghostscript gsfonts\n",
            "  imagemagick-6-common imagemagick-6.q16 libdjvulibre-text libdjvulibre21\n",
            "  libgs9 libgs9-common libidn11 libijs-0.35 libjbig2dec0 liblqr-1-0\n",
            "  libmagickcore-6.q16-6 libmagickcore-6.q16-6-extra libmagickwand-6.q16-6\n",
            "  libnetpbm10 libwmf0.2-7 netpbm poppler-data\n",
            "Suggested packages:\n",
            "  fonts-noto fonts-freefont-otf | fonts-freefont-ttf fonts-texgyre\n",
            "  ghostscript-x imagemagick-doc autotrace cups-bsd | lpr | lprng enscript gimp\n",
            "  gnuplot grads hp2xx html2ps libwmf-bin mplayer povray radiance sane-utils\n",
            "  texlive-base-bin transfig ufraw-batch inkscape libjxr-tools libwmf0.2-7-gtk\n",
            "  poppler-utils fonts-japanese-mincho | fonts-ipafont-mincho\n",
            "  fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n",
            "  fonts-arphic-uming fonts-nanum\n",
            "The following NEW packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 ghostscript gsfonts\n",
            "  imagemagick imagemagick-6-common imagemagick-6.q16 libdjvulibre-text\n",
            "  libdjvulibre21 libgs9 libgs9-common libidn11 libijs-0.35 libjbig2dec0\n",
            "  liblqr-1-0 libmagickcore-6.q16-6 libmagickcore-6.q16-6-extra\n",
            "  libmagickwand-6.q16-6 libnetpbm10 libwmf0.2-7 netpbm poppler-data\n",
            "0 upgraded, 23 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 20.2 MB of archives.\n",
            "After this operation, 73.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 liblqr-1-0 amd64 0.4.2-2.1 [27.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 imagemagick-6-common all 8:6.9.10.23+dfsg-2.1ubuntu11.4 [60.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libmagickcore-6.q16-6 amd64 8:6.9.10.23+dfsg-2.1ubuntu11.4 [1,647 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libmagickwand-6.q16-6 amd64 8:6.9.10.23+dfsg-2.1ubuntu11.4 [303 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 poppler-data all 0.4.9-2 [1,475 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 fonts-noto-mono all 20200323-1build1~ubuntu20.04.1 [80.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-urw-base35 all 20170801.1-3 [6,333 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgs9-common all 9.50~dfsg-5ubuntu4.6 [681 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 libidn11 amd64 1.33-2.2ubuntu2 [46.2 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libijs-0.35 amd64 0.35-15 [15.7 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libjbig2dec0 amd64 0.18-1ubuntu1 [60.0 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgs9 amd64 9.50~dfsg-5ubuntu4.6 [2,173 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 ghostscript amd64 9.50~dfsg-5ubuntu4.6 [51.8 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal/universe amd64 gsfonts all 1:8.11+urwcyr1.0.7~pre44-4.4 [3,120 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 imagemagick-6.q16 amd64 8:6.9.10.23+dfsg-2.1ubuntu11.4 [427 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 imagemagick amd64 8:6.9.10.23+dfsg-2.1ubuntu11.4 [14.4 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libdjvulibre-text all 3.5.27.1-14ubuntu0.1 [49.2 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libdjvulibre21 amd64 3.5.27.1-14ubuntu0.1 [578 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu focal/main amd64 libwmf0.2-7 amd64 0.2.8.4-17ubuntu1 [149 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libmagickcore-6.q16-6-extra amd64 8:6.9.10.23+dfsg-2.1ubuntu11.4 [64.6 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu focal/universe amd64 libnetpbm10 amd64 2:10.0-15.3build1 [58.0 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu focal/universe amd64 netpbm amd64 2:10.0-15.3build1 [1,017 kB]\n",
            "Fetched 20.2 MB in 1s (17.0 MB/s)\n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 128208 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Selecting previously unselected package liblqr-1-0:amd64.\n",
            "Preparing to unpack .../01-liblqr-1-0_0.4.2-2.1_amd64.deb ...\n",
            "Unpacking liblqr-1-0:amd64 (0.4.2-2.1) ...\n",
            "Selecting previously unselected package imagemagick-6-common.\n",
            "Preparing to unpack .../02-imagemagick-6-common_8%3a6.9.10.23+dfsg-2.1ubuntu11.4_all.deb ...\n",
            "Unpacking imagemagick-6-common (8:6.9.10.23+dfsg-2.1ubuntu11.4) ...\n",
            "Selecting previously unselected package libmagickcore-6.q16-6:amd64.\n",
            "Preparing to unpack .../03-libmagickcore-6.q16-6_8%3a6.9.10.23+dfsg-2.1ubuntu11.4_amd64.deb ...\n",
            "Unpacking libmagickcore-6.q16-6:amd64 (8:6.9.10.23+dfsg-2.1ubuntu11.4) ...\n",
            "Selecting previously unselected package libmagickwand-6.q16-6:amd64.\n",
            "Preparing to unpack .../04-libmagickwand-6.q16-6_8%3a6.9.10.23+dfsg-2.1ubuntu11.4_amd64.deb ...\n",
            "Unpacking libmagickwand-6.q16-6:amd64 (8:6.9.10.23+dfsg-2.1ubuntu11.4) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../05-poppler-data_0.4.9-2_all.deb ...\n",
            "Unpacking poppler-data (0.4.9-2) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../06-fonts-noto-mono_20200323-1build1~ubuntu20.04.1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20200323-1build1~ubuntu20.04.1) ...\n",
            "Selecting previously unselected package fonts-urw-base35.\n",
            "Preparing to unpack .../07-fonts-urw-base35_20170801.1-3_all.deb ...\n",
            "Unpacking fonts-urw-base35 (20170801.1-3) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../08-libgs9-common_9.50~dfsg-5ubuntu4.6_all.deb ...\n",
            "Unpacking libgs9-common (9.50~dfsg-5ubuntu4.6) ...\n",
            "Selecting previously unselected package libidn11:amd64.\n",
            "Preparing to unpack .../09-libidn11_1.33-2.2ubuntu2_amd64.deb ...\n",
            "Unpacking libidn11:amd64 (1.33-2.2ubuntu2) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../10-libijs-0.35_0.35-15_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-15) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../11-libjbig2dec0_0.18-1ubuntu1_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.18-1ubuntu1) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../12-libgs9_9.50~dfsg-5ubuntu4.6_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.50~dfsg-5ubuntu4.6) ...\n",
            "Selecting previously unselected package ghostscript.\n",
            "Preparing to unpack .../13-ghostscript_9.50~dfsg-5ubuntu4.6_amd64.deb ...\n",
            "Unpacking ghostscript (9.50~dfsg-5ubuntu4.6) ...\n",
            "Selecting previously unselected package gsfonts.\n",
            "Preparing to unpack .../14-gsfonts_1%3a8.11+urwcyr1.0.7~pre44-4.4_all.deb ...\n",
            "Unpacking gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Selecting previously unselected package imagemagick-6.q16.\n",
            "Preparing to unpack .../15-imagemagick-6.q16_8%3a6.9.10.23+dfsg-2.1ubuntu11.4_amd64.deb ...\n",
            "Unpacking imagemagick-6.q16 (8:6.9.10.23+dfsg-2.1ubuntu11.4) ...\n",
            "Selecting previously unselected package imagemagick.\n",
            "Preparing to unpack .../16-imagemagick_8%3a6.9.10.23+dfsg-2.1ubuntu11.4_amd64.deb ...\n",
            "Unpacking imagemagick (8:6.9.10.23+dfsg-2.1ubuntu11.4) ...\n",
            "Selecting previously unselected package libdjvulibre-text.\n",
            "Preparing to unpack .../17-libdjvulibre-text_3.5.27.1-14ubuntu0.1_all.deb ...\n",
            "Unpacking libdjvulibre-text (3.5.27.1-14ubuntu0.1) ...\n",
            "Selecting previously unselected package libdjvulibre21:amd64.\n",
            "Preparing to unpack .../18-libdjvulibre21_3.5.27.1-14ubuntu0.1_amd64.deb ...\n",
            "Unpacking libdjvulibre21:amd64 (3.5.27.1-14ubuntu0.1) ...\n",
            "Selecting previously unselected package libwmf0.2-7:amd64.\n",
            "Preparing to unpack .../19-libwmf0.2-7_0.2.8.4-17ubuntu1_amd64.deb ...\n",
            "Unpacking libwmf0.2-7:amd64 (0.2.8.4-17ubuntu1) ...\n",
            "Selecting previously unselected package libmagickcore-6.q16-6-extra:amd64.\n",
            "Preparing to unpack .../20-libmagickcore-6.q16-6-extra_8%3a6.9.10.23+dfsg-2.1ubuntu11.4_amd64.deb ...\n",
            "Unpacking libmagickcore-6.q16-6-extra:amd64 (8:6.9.10.23+dfsg-2.1ubuntu11.4) ...\n",
            "Selecting previously unselected package libnetpbm10.\n",
            "Preparing to unpack .../21-libnetpbm10_2%3a10.0-15.3build1_amd64.deb ...\n",
            "Unpacking libnetpbm10 (2:10.0-15.3build1) ...\n",
            "Selecting previously unselected package netpbm.\n",
            "Preparing to unpack .../22-netpbm_2%3a10.0-15.3build1_amd64.deb ...\n",
            "Unpacking netpbm (2:10.0-15.3build1) ...\n",
            "Setting up imagemagick-6-common (8:6.9.10.23+dfsg-2.1ubuntu11.4) ...\n",
            "Setting up fonts-noto-mono (20200323-1build1~ubuntu20.04.1) ...\n",
            "Setting up libwmf0.2-7:amd64 (0.2.8.4-17ubuntu1) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-15) ...\n",
            "Setting up libnetpbm10 (2:10.0-15.3build1) ...\n",
            "Setting up fonts-urw-base35 (20170801.1-3) ...\n",
            "Setting up poppler-data (0.4.9-2) ...\n",
            "Setting up libjbig2dec0:amd64 (0.18-1ubuntu1) ...\n",
            "Setting up libidn11:amd64 (1.33-2.2ubuntu2) ...\n",
            "Setting up gsfonts (1:8.11+urwcyr1.0.7~pre44-4.4) ...\n",
            "Setting up netpbm (2:10.0-15.3build1) ...\n",
            "Setting up liblqr-1-0:amd64 (0.4.2-2.1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Setting up libdjvulibre-text (3.5.27.1-14ubuntu0.1) ...\n",
            "Setting up libgs9-common (9.50~dfsg-5ubuntu4.6) ...\n",
            "Setting up libgs9:amd64 (9.50~dfsg-5ubuntu4.6) ...\n",
            "Setting up libdjvulibre21:amd64 (3.5.27.1-14ubuntu0.1) ...\n",
            "Setting up ghostscript (9.50~dfsg-5ubuntu4.6) ...\n",
            "Setting up libmagickcore-6.q16-6:amd64 (8:6.9.10.23+dfsg-2.1ubuntu11.4) ...\n",
            "Setting up libmagickwand-6.q16-6:amd64 (8:6.9.10.23+dfsg-2.1ubuntu11.4) ...\n",
            "Setting up libmagickcore-6.q16-6-extra:amd64 (8:6.9.10.23+dfsg-2.1ubuntu11.4) ...\n",
            "Setting up imagemagick-6.q16 (8:6.9.10.23+dfsg-2.1ubuntu11.4) ...\n",
            "update-alternatives: using /usr/bin/compare-im6.q16 to provide /usr/bin/compare (compare) in auto mode\n",
            "update-alternatives: using /usr/bin/compare-im6.q16 to provide /usr/bin/compare-im6 (compare-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/animate-im6.q16 to provide /usr/bin/animate (animate) in auto mode\n",
            "update-alternatives: using /usr/bin/animate-im6.q16 to provide /usr/bin/animate-im6 (animate-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/convert-im6.q16 to provide /usr/bin/convert (convert) in auto mode\n",
            "update-alternatives: using /usr/bin/convert-im6.q16 to provide /usr/bin/convert-im6 (convert-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/composite-im6.q16 to provide /usr/bin/composite (composite) in auto mode\n",
            "update-alternatives: using /usr/bin/composite-im6.q16 to provide /usr/bin/composite-im6 (composite-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/conjure-im6.q16 to provide /usr/bin/conjure (conjure) in auto mode\n",
            "update-alternatives: using /usr/bin/conjure-im6.q16 to provide /usr/bin/conjure-im6 (conjure-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/import-im6.q16 to provide /usr/bin/import (import) in auto mode\n",
            "update-alternatives: using /usr/bin/import-im6.q16 to provide /usr/bin/import-im6 (import-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/identify-im6.q16 to provide /usr/bin/identify (identify) in auto mode\n",
            "update-alternatives: using /usr/bin/identify-im6.q16 to provide /usr/bin/identify-im6 (identify-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/stream-im6.q16 to provide /usr/bin/stream (stream) in auto mode\n",
            "update-alternatives: using /usr/bin/stream-im6.q16 to provide /usr/bin/stream-im6 (stream-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/display-im6.q16 to provide /usr/bin/display (display) in auto mode\n",
            "update-alternatives: using /usr/bin/display-im6.q16 to provide /usr/bin/display-im6 (display-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/montage-im6.q16 to provide /usr/bin/montage (montage) in auto mode\n",
            "update-alternatives: using /usr/bin/montage-im6.q16 to provide /usr/bin/montage-im6 (montage-im6) in auto mode\n",
            "update-alternatives: using /usr/bin/mogrify-im6.q16 to provide /usr/bin/mogrify (mogrify) in auto mode\n",
            "update-alternatives: using /usr/bin/mogrify-im6.q16 to provide /usr/bin/mogrify-im6 (mogrify-im6) in auto mode\n",
            "Setting up imagemagick (8:6.9.10.23+dfsg-2.1ubuntu11.4) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mogrify -format jpg /content/eggs_pascal_voc/JPEGImages/*.bmp "
      ],
      "metadata": {
        "id": "07CBTGvVfwDW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/eggs_pascal_voc/JPEGImages/*.bmp "
      ],
      "metadata": {
        "id": "3aVxmySvgOQG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/eggs_pascal_voc/JPEGImages/* /content/eggs_pascal_voc/labels_and_images/"
      ],
      "metadata": {
        "id": "xpxLmIKFkONd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/eggs_pascal_voc/Annotations/* /content/eggs_pascal_voc/labels_and_images/"
      ],
      "metadata": {
        "id": "C51EnIcukXIZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/eggs_pascal_voc/labels_and_images/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7YTs-kxkT-e",
        "outputId": "fc8f627d-61f6-4dd7-9306-ca0ec98eda38"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20220928-WA0008-100.jpg  20220928-WA0008-147.xml  20220928-WA0008-54.jpg\n",
            "20220928-WA0008-100.xml  20220928-WA0008-148.jpg  20220928-WA0008-54.xml\n",
            "20220928-WA0008-101.jpg  20220928-WA0008-148.xml  20220928-WA0008-55.jpg\n",
            "20220928-WA0008-101.xml  20220928-WA0008-149.jpg  20220928-WA0008-55.xml\n",
            "20220928-WA0008-102.jpg  20220928-WA0008-149.xml  20220928-WA0008-56.jpg\n",
            "20220928-WA0008-102.xml  20220928-WA0008-14.jpg   20220928-WA0008-56.xml\n",
            "20220928-WA0008-103.jpg  20220928-WA0008-14.xml   20220928-WA0008-57.jpg\n",
            "20220928-WA0008-103.xml  20220928-WA0008-150.jpg  20220928-WA0008-57.xml\n",
            "20220928-WA0008-104.jpg  20220928-WA0008-150.xml  20220928-WA0008-58.jpg\n",
            "20220928-WA0008-104.xml  20220928-WA0008-151.jpg  20220928-WA0008-58.xml\n",
            "20220928-WA0008-105.jpg  20220928-WA0008-151.xml  20220928-WA0008-59.jpg\n",
            "20220928-WA0008-105.xml  20220928-WA0008-152.jpg  20220928-WA0008-59.xml\n",
            "20220928-WA0008-106.jpg  20220928-WA0008-152.xml  20220928-WA0008-5.jpg\n",
            "20220928-WA0008-106.xml  20220928-WA0008-153.jpg  20220928-WA0008-5.xml\n",
            "20220928-WA0008-107.jpg  20220928-WA0008-153.xml  20220928-WA0008-60.jpg\n",
            "20220928-WA0008-107.xml  20220928-WA0008-154.jpg  20220928-WA0008-60.xml\n",
            "20220928-WA0008-108.jpg  20220928-WA0008-154.xml  20220928-WA0008-61.jpg\n",
            "20220928-WA0008-108.xml  20220928-WA0008-15.jpg   20220928-WA0008-61.xml\n",
            "20220928-WA0008-109.jpg  20220928-WA0008-15.xml   20220928-WA0008-62.jpg\n",
            "20220928-WA0008-109.xml  20220928-WA0008-16.jpg   20220928-WA0008-62.xml\n",
            "20220928-WA0008-10.jpg\t 20220928-WA0008-16.xml   20220928-WA0008-63.jpg\n",
            "20220928-WA0008-10.xml\t 20220928-WA0008-17.jpg   20220928-WA0008-63.xml\n",
            "20220928-WA0008-110.jpg  20220928-WA0008-17.xml   20220928-WA0008-64.jpg\n",
            "20220928-WA0008-110.xml  20220928-WA0008-18.jpg   20220928-WA0008-64.xml\n",
            "20220928-WA0008-111.jpg  20220928-WA0008-18.xml   20220928-WA0008-65.jpg\n",
            "20220928-WA0008-111.xml  20220928-WA0008-19.jpg   20220928-WA0008-65.xml\n",
            "20220928-WA0008-112.jpg  20220928-WA0008-19.xml   20220928-WA0008-66.jpg\n",
            "20220928-WA0008-112.xml  20220928-WA0008-1.jpg\t  20220928-WA0008-66.xml\n",
            "20220928-WA0008-113.jpg  20220928-WA0008-1.xml\t  20220928-WA0008-67.jpg\n",
            "20220928-WA0008-113.xml  20220928-WA0008-20.jpg   20220928-WA0008-67.xml\n",
            "20220928-WA0008-114.jpg  20220928-WA0008-20.xml   20220928-WA0008-68.jpg\n",
            "20220928-WA0008-114.xml  20220928-WA0008-21.jpg   20220928-WA0008-68.xml\n",
            "20220928-WA0008-115.jpg  20220928-WA0008-21.xml   20220928-WA0008-69.jpg\n",
            "20220928-WA0008-115.xml  20220928-WA0008-22.jpg   20220928-WA0008-69.xml\n",
            "20220928-WA0008-116.jpg  20220928-WA0008-22.xml   20220928-WA0008-6.jpg\n",
            "20220928-WA0008-116.xml  20220928-WA0008-23.jpg   20220928-WA0008-6.xml\n",
            "20220928-WA0008-117.jpg  20220928-WA0008-23.xml   20220928-WA0008-70.jpg\n",
            "20220928-WA0008-117.xml  20220928-WA0008-24.jpg   20220928-WA0008-70.xml\n",
            "20220928-WA0008-118.jpg  20220928-WA0008-24.xml   20220928-WA0008-71.jpg\n",
            "20220928-WA0008-118.xml  20220928-WA0008-25.jpg   20220928-WA0008-71.xml\n",
            "20220928-WA0008-119.jpg  20220928-WA0008-25.xml   20220928-WA0008-72.jpg\n",
            "20220928-WA0008-119.xml  20220928-WA0008-26.jpg   20220928-WA0008-72.xml\n",
            "20220928-WA0008-11.jpg\t 20220928-WA0008-26.xml   20220928-WA0008-73.jpg\n",
            "20220928-WA0008-11.xml\t 20220928-WA0008-27.jpg   20220928-WA0008-73.xml\n",
            "20220928-WA0008-120.jpg  20220928-WA0008-27.xml   20220928-WA0008-74.jpg\n",
            "20220928-WA0008-120.xml  20220928-WA0008-28.jpg   20220928-WA0008-74.xml\n",
            "20220928-WA0008-121.jpg  20220928-WA0008-28.xml   20220928-WA0008-75.jpg\n",
            "20220928-WA0008-121.xml  20220928-WA0008-29.jpg   20220928-WA0008-75.xml\n",
            "20220928-WA0008-122.jpg  20220928-WA0008-29.xml   20220928-WA0008-76.jpg\n",
            "20220928-WA0008-122.xml  20220928-WA0008-2.jpg\t  20220928-WA0008-76.xml\n",
            "20220928-WA0008-123.jpg  20220928-WA0008-2.xml\t  20220928-WA0008-77.jpg\n",
            "20220928-WA0008-123.xml  20220928-WA0008-30.jpg   20220928-WA0008-77.xml\n",
            "20220928-WA0008-124.jpg  20220928-WA0008-30.xml   20220928-WA0008-78.jpg\n",
            "20220928-WA0008-124.xml  20220928-WA0008-31.jpg   20220928-WA0008-78.xml\n",
            "20220928-WA0008-125.jpg  20220928-WA0008-31.xml   20220928-WA0008-79.jpg\n",
            "20220928-WA0008-125.xml  20220928-WA0008-32.jpg   20220928-WA0008-79.xml\n",
            "20220928-WA0008-126.jpg  20220928-WA0008-32.xml   20220928-WA0008-7.jpg\n",
            "20220928-WA0008-126.xml  20220928-WA0008-33.jpg   20220928-WA0008-7.xml\n",
            "20220928-WA0008-127.jpg  20220928-WA0008-33.xml   20220928-WA0008-80.jpg\n",
            "20220928-WA0008-127.xml  20220928-WA0008-34.jpg   20220928-WA0008-80.xml\n",
            "20220928-WA0008-128.jpg  20220928-WA0008-34.xml   20220928-WA0008-81.jpg\n",
            "20220928-WA0008-128.xml  20220928-WA0008-35.jpg   20220928-WA0008-81.xml\n",
            "20220928-WA0008-129.jpg  20220928-WA0008-35.xml   20220928-WA0008-82.jpg\n",
            "20220928-WA0008-129.xml  20220928-WA0008-36.jpg   20220928-WA0008-82.xml\n",
            "20220928-WA0008-12.jpg\t 20220928-WA0008-36.xml   20220928-WA0008-83.jpg\n",
            "20220928-WA0008-12.xml\t 20220928-WA0008-37.jpg   20220928-WA0008-83.xml\n",
            "20220928-WA0008-130.jpg  20220928-WA0008-37.xml   20220928-WA0008-84.jpg\n",
            "20220928-WA0008-130.xml  20220928-WA0008-38.jpg   20220928-WA0008-84.xml\n",
            "20220928-WA0008-131.jpg  20220928-WA0008-38.xml   20220928-WA0008-85.jpg\n",
            "20220928-WA0008-131.xml  20220928-WA0008-39.jpg   20220928-WA0008-85.xml\n",
            "20220928-WA0008-132.jpg  20220928-WA0008-39.xml   20220928-WA0008-86.jpg\n",
            "20220928-WA0008-132.xml  20220928-WA0008-3.jpg\t  20220928-WA0008-86.xml\n",
            "20220928-WA0008-133.jpg  20220928-WA0008-3.xml\t  20220928-WA0008-87.jpg\n",
            "20220928-WA0008-133.xml  20220928-WA0008-40.jpg   20220928-WA0008-87.xml\n",
            "20220928-WA0008-134.jpg  20220928-WA0008-40.xml   20220928-WA0008-88.jpg\n",
            "20220928-WA0008-134.xml  20220928-WA0008-41.jpg   20220928-WA0008-88.xml\n",
            "20220928-WA0008-135.jpg  20220928-WA0008-41.xml   20220928-WA0008-89.jpg\n",
            "20220928-WA0008-135.xml  20220928-WA0008-42.jpg   20220928-WA0008-89.xml\n",
            "20220928-WA0008-136.jpg  20220928-WA0008-42.xml   20220928-WA0008-8.jpg\n",
            "20220928-WA0008-136.xml  20220928-WA0008-43.jpg   20220928-WA0008-8.xml\n",
            "20220928-WA0008-137.jpg  20220928-WA0008-43.xml   20220928-WA0008-90.jpg\n",
            "20220928-WA0008-137.xml  20220928-WA0008-44.jpg   20220928-WA0008-90.xml\n",
            "20220928-WA0008-138.jpg  20220928-WA0008-44.xml   20220928-WA0008-91.jpg\n",
            "20220928-WA0008-138.xml  20220928-WA0008-45.jpg   20220928-WA0008-91.xml\n",
            "20220928-WA0008-139.jpg  20220928-WA0008-45.xml   20220928-WA0008-92.jpg\n",
            "20220928-WA0008-139.xml  20220928-WA0008-46.jpg   20220928-WA0008-92.xml\n",
            "20220928-WA0008-13.jpg\t 20220928-WA0008-46.xml   20220928-WA0008-93.jpg\n",
            "20220928-WA0008-13.xml\t 20220928-WA0008-47.jpg   20220928-WA0008-93.xml\n",
            "20220928-WA0008-140.jpg  20220928-WA0008-47.xml   20220928-WA0008-94.jpg\n",
            "20220928-WA0008-140.xml  20220928-WA0008-48.jpg   20220928-WA0008-94.xml\n",
            "20220928-WA0008-141.jpg  20220928-WA0008-48.xml   20220928-WA0008-95.jpg\n",
            "20220928-WA0008-141.xml  20220928-WA0008-49.jpg   20220928-WA0008-95.xml\n",
            "20220928-WA0008-142.jpg  20220928-WA0008-49.xml   20220928-WA0008-96.jpg\n",
            "20220928-WA0008-142.xml  20220928-WA0008-4.jpg\t  20220928-WA0008-96.xml\n",
            "20220928-WA0008-143.jpg  20220928-WA0008-4.xml\t  20220928-WA0008-97.jpg\n",
            "20220928-WA0008-143.xml  20220928-WA0008-50.jpg   20220928-WA0008-97.xml\n",
            "20220928-WA0008-144.jpg  20220928-WA0008-50.xml   20220928-WA0008-98.jpg\n",
            "20220928-WA0008-144.xml  20220928-WA0008-51.jpg   20220928-WA0008-98.xml\n",
            "20220928-WA0008-145.jpg  20220928-WA0008-51.xml   20220928-WA0008-99.jpg\n",
            "20220928-WA0008-145.xml  20220928-WA0008-52.jpg   20220928-WA0008-99.xml\n",
            "20220928-WA0008-146.jpg  20220928-WA0008-52.xml   20220928-WA0008-9.jpg\n",
            "20220928-WA0008-146.xml  20220928-WA0008-53.jpg   20220928-WA0008-9.xml\n",
            "20220928-WA0008-147.jpg  20220928-WA0008-53.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -5 /content/eggs_pascal_voc/labels_and_images/20220928-WA0008-1.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXKFLPQzhpWR",
        "outputId": "85f610c5-46d6-4b77-8ca5-36a3e4b9d5cc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<annotation>\n",
            "  <folder></folder>\n",
            "  <filename>20220928-WA0008-1.bmp</filename>\n",
            "  <source>\n",
            "    <database>Unknown</database>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/bmp/jpg/g' /content/eggs_pascal_voc/labels_and_images/*.xml"
      ],
      "metadata": {
        "id": "dCkEAk0TgnUe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/<\\/name>/<\\/name><pose>Unspecified<\\/pose>/g' /content/eggs_pascal_voc/labels_and_images/*.xml "
      ],
      "metadata": {
        "id": "aYPhKZYRjJ2Y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/\\.[0-9]*//g' /content/eggs_pascal_voc/labels_and_images/*.xml "
      ],
      "metadata": {
        "id": "wUDoHKlPlacW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/jpg/\\.jpg/g' /content/eggs_pascal_voc/labels_and_images/*.xml "
      ],
      "metadata": {
        "id": "WQfGdRu1whMq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -5 /content/eggs_pascal_voc/labels_and_images/20220928-WA0008-1.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTnvnZtBh11d",
        "outputId": "815e5ea2-12a2-45d0-87a3-f9e6d99108dc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<annotation>\n",
            "  <folder></folder>\n",
            "  <filename>20220928-WA0008-1.jpg</filename>\n",
            "  <source>\n",
            "    <database>Unknown</database>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/eggs_pascal_voc/labels_and_images/20220928-WA0008-1.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3zxJalZi51m",
        "outputId": "877431b4-f630-4a27-fd82-4cd04e09add3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<annotation>\n",
            "  <folder></folder>\n",
            "  <filename>20220928-WA0008-1.jpg</filename>\n",
            "  <source>\n",
            "    <database>Unknown</database>\n",
            "    <annotation>Unknown</annotation>\n",
            "    <image>Unknown</image>\n",
            "  </source>\n",
            "  <size>\n",
            "    <width>640</width>\n",
            "    <height>352</height>\n",
            "    <depth></depth>\n",
            "  </size>\n",
            "  <segmented>0</segmented>\n",
            "  <object>\n",
            "    <name>a_yolk</name><pose>Unspecified</pose>\n",
            "    <truncated>0</truncated>\n",
            "    <occluded>0</occluded>\n",
            "    <difficult>0</difficult>\n",
            "    <bndbox>\n",
            "      <xmin>0</xmin>\n",
            "      <ymin>118</ymin>\n",
            "      <xmax>101</xmax>\n",
            "      <ymax>292</ymax>\n",
            "    </bndbox>\n",
            "    <attributes>\n",
            "      <attribute>\n",
            "        <name>rotation</name><pose>Unspecified</pose>\n",
            "        <value>0</value>\n",
            "      </attribute>\n",
            "    </attributes>\n",
            "  </object>\n",
            "  <object>\n",
            "    <name>b_yolk</name><pose>Unspecified</pose>\n",
            "    <truncated>0</truncated>\n",
            "    <occluded>0</occluded>\n",
            "    <difficult>0</difficult>\n",
            "    <bndbox>\n",
            "      <xmin>459</xmin>\n",
            "      <ymin>127</ymin>\n",
            "      <xmax>640</xmax>\n",
            "      <ymax>313</ymax>\n",
            "    </bndbox>\n",
            "    <attributes>\n",
            "      <attribute>\n",
            "        <name>rotation</name><pose>Unspecified</pose>\n",
            "        <value>0</value>\n",
            "      </attribute>\n",
            "    </attributes>\n",
            "  </object>\n",
            "</annotation>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat  /content/android_figurine/train/IMG_0509.xml "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ3Me32viDn4",
        "outputId": "5c8ef148-ee7c-40cb-f45b-4689132ab0ce"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<annotation verified=\"yes\">\n",
            "\t<folder>train</folder>\n",
            "\t<filename>IMG_0509.jpg</filename>\n",
            "\t<path>/Users/thuytran/Downloads/Photos-002/train/IMG_0509.jpg</path>\n",
            "\t<source>\n",
            "\t\t<database>Unknown</database>\n",
            "\t</source>\n",
            "\t<size>\n",
            "\t\t<width>800</width>\n",
            "\t\t<height>600</height>\n",
            "\t\t<depth>3</depth>\n",
            "\t</size>\n",
            "\t<segmented>0</segmented>\n",
            "\t<object>\n",
            "\t\t<name>android</name>\n",
            "\t\t<pose>Unspecified</pose>\n",
            "\t\t<truncated>0</truncated>\n",
            "\t\t<difficult>0</difficult>\n",
            "\t\t<bndbox>\n",
            "\t\t\t<xmin>10</xmin>\n",
            "\t\t\t<ymin>131</ymin>\n",
            "\t\t\t<xmax>288</xmax>\n",
            "\t\t\t<ymax>401</ymax>\n",
            "\t\t</bndbox>\n",
            "\t</object>\n",
            "\t<object>\n",
            "\t\t<name>pig_android</name>\n",
            "\t\t<pose>Unspecified</pose>\n",
            "\t\t<truncated>0</truncated>\n",
            "\t\t<difficult>0</difficult>\n",
            "\t\t<bndbox>\n",
            "\t\t\t<xmin>526</xmin>\n",
            "\t\t\t<ymin>82</ymin>\n",
            "\t\t\t<xmax>722</xmax>\n",
            "\t\t\t<ymax>324</ymax>\n",
            "\t\t</bndbox>\n",
            "\t</object>\n",
            "</annotation>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pr -m -t /content/android_figurine/train/IMG_0509.xml /content/eggs_pascal_voc/labels_and_images/20220928-WA0008-1.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DWwF3rNijjI",
        "outputId": "9bfbcd10-4595-4097-fbb0-6cb4196d0557"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<annotation verified=\"yes\">\t    <annotation>\n",
            "\t<folder>train</folder>\t\t      <folder></folder>\n",
            "\t<filename>IMG_0509.jpg</fil\t      <filename>20220928-WA0008-1.jpg</\n",
            "\t<path>/Users/thuytran/Downl\t      <source>\n",
            "\t<source>\t\t\t    \t<database>Unknown</database>\n",
            "\t\t<database>Unknown</\t\t    \t<annotation>Unknown</annotation\n",
            "\t</source>\t\t\t    \t<image>Unknown</image>\n",
            "\t<size>\t\t\t\t      </source>\n",
            "\t\t<width>800</width>\t\t      <size>\n",
            "\t\t<height>600</height\t\t    \t<width>640</width>\n",
            "\t\t<depth>3</depth>\t\t    \t<height>352</height>\n",
            "\t</size>\t\t\t\t    \t<depth></depth>\n",
            "\t<segmented>0</segmented>\t      </size>\n",
            "\t<object>\t\t\t      <segmented>0</segmented>\n",
            "\t\t<name>android</name\t\t      <object>\n",
            "\t\t<pose>Unspecified</\t\t    \t<name>a_yolk</name><pose>Unspec\n",
            "\t\t<truncated>0</trunc\t\t    \t<truncated>0</truncated>\n",
            "\t\t<difficult>0</diffi\t\t    \t<occluded>0</occluded>\n",
            "\t\t<bndbox>\t\t\t    \t<difficult>0</difficult>\n",
            "\t\t\t<xmin>10</x\t\t\t    \t<bndbox>\n",
            "\t\t\t<ymin>131</\t\t\t    \t  <xmin>0</xmin>\n",
            "\t\t\t<xmax>288</\t\t\t    \t  <ymin>118</ymin>\n",
            "\t\t\t<ymax>401</\t\t\t    \t  <xmax>101</xmax>\n",
            "\t\t</bndbox>\t\t\t    \t  <ymax>292</ymax>\n",
            "\t</object>\t\t\t    \t</bndbox>\n",
            "\t<object>\t\t\t    \t<attributes>\n",
            "\t\t<name>pig_android</\t\t    \t  <attribute>\n",
            "\t\t<pose>Unspecified</\t\t    \t    <name>rotation</name><pose>\n",
            "\t\t<truncated>0</trunc\t\t    \t    <value>0</value>\n",
            "\t\t<difficult>0</diffi\t\t    \t  </attribute>\n",
            "\t\t<bndbox>\t\t\t    \t</attributes>\n",
            "\t\t\t<xmin>526</\t\t\t      </object>\n",
            "\t\t\t<ymin>82</y\t\t\t      <object>\n",
            "\t\t\t<xmax>722</\t\t\t    \t<name>b_yolk</name><pose>Unspec\n",
            "\t\t\t<ymax>324</\t\t\t    \t<truncated>0</truncated>\n",
            "\t\t</bndbox>\t\t\t    \t<occluded>0</occluded>\n",
            "\t</object>\t\t\t    \t<difficult>0</difficult>\n",
            "</annotation>\t\t\t    \t<bndbox>\n",
            "\t\t\t\t    \t  <xmin>459</xmin>\n",
            "\t\t\t\t    \t  <ymin>127</ymin>\n",
            "\t\t\t\t    \t  <xmax>640</xmax>\n",
            "\t\t\t\t    \t  <ymax>313</ymax>\n",
            "\t\t\t\t    \t</bndbox>\n",
            "\t\t\t\t    \t<attributes>\n",
            "\t\t\t\t    \t  <attribute>\n",
            "\t\t\t\t    \t    <name>rotation</name><pose>\n",
            "\t\t\t\t    \t    <value>0</value>\n",
            "\t\t\t\t    \t  </attribute>\n",
            "\t\t\t\t    \t</attributes>\n",
            "\t\t\t\t      </object>\n",
            "\t\t\t\t    </annotation>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxh3KInCFeB-"
      },
      "source": [
        "## Train the object detection model\n",
        "\n",
        "### Step 1: Load the dataset\n",
        "\n",
        "* Images in `train_data` is used to train the custom object detection model.\n",
        "* Images in `val_data` is used to check if the model can generalize well to new images that it hasn't seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WiAahdsQAdT7"
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    '/content/eggs_pascal_voc/labels_and_images',\n",
        "    '/content/eggs_pascal_voc/labels_and_images',\n",
        "    ['a_yolk', 'b_yolk', 'c_yolk']\n",
        ")\n",
        "\n",
        "val_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    '/content/eggs_pascal_voc/labels_and_images',\n",
        "    '/content/eggs_pascal_voc/labels_and_images',\n",
        "    ['a_yolk', 'b_yolk', 'c_yolk']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNRhB8N7GHXj"
      },
      "source": [
        "### Step 2: Select a model architecture\n",
        "\n",
        "EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture.\n",
        "\n",
        "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
        "\n",
        "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|--------------------|-----------|---------------|----------------------|\n",
        "| EfficientDet-Lite0 | 4.4       | 146           | 25.69%               |\n",
        "| EfficientDet-Lite1 | 5.8       | 259           | 30.55%               |\n",
        "| EfficientDet-Lite2 | 7.2       | 396           | 33.97%               |\n",
        "| EfficientDet-Lite3 | 11.4      | 716           | 37.70%               |\n",
        "| EfficientDet-Lite4 | 19.9      | 1886          | 41.96%               |\n",
        "\n",
        "<i> * Size of the integer quantized models. <br/>\n",
        "** Latency measured on Raspberry Pi 4 using 4 threads on CPU. <br/>\n",
        "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
        "</i>\n",
        "\n",
        "In this notebook, we use EfficientDet-Lite0 to train our model. You can choose other model architectures depending on whether speed or accuracy is more important to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GZOojrDHAY1J"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('efficientdet_lite0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aeDU4mIM4ft"
      },
      "source": [
        "### Step 3: Train the TensorFlow model with the training data.\n",
        "\n",
        "* Set `epochs = 20`, which means it will go through the training dataset 20 times. You can look at the validation accuracy during training and stop when you see validation loss (`val_loss`) stop decreasing to avoid overfitting.\n",
        "* Set `batch_size = 4` here so you will see that it takes 15 steps to go through the 62 images in the training dataset.\n",
        "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#batch 64 works too, 14-15gb/16"
      ],
      "metadata": {
        "id": "PwTjikDHy5by"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MClfpsJAfda",
        "outputId": "8f132530-8893-4014-dce4-4dabc5e2dfa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "4/4 [==============================] - 156s 30s/step - det_loss: 1.8161 - cls_loss: 1.1366 - box_loss: 0.0136 - reg_l2_loss: 0.0632 - loss: 1.8793 - learning_rate: 0.0224 - gradient_norm: 1.0069 - val_det_loss: 1.7182 - val_cls_loss: 1.0882 - val_box_loss: 0.0126 - val_reg_l2_loss: 0.0632 - val_loss: 1.7814\n",
            "Epoch 2/50\n",
            "4/4 [==============================] - 108s 29s/step - det_loss: 1.6446 - cls_loss: 1.0630 - box_loss: 0.0116 - reg_l2_loss: 0.0632 - loss: 1.7078 - learning_rate: 0.0399 - gradient_norm: 1.0656 - val_det_loss: 1.4093 - val_cls_loss: 0.8993 - val_box_loss: 0.0102 - val_reg_l2_loss: 0.0632 - val_loss: 1.4725\n",
            "Epoch 3/50\n",
            "4/4 [==============================] - 107s 29s/step - det_loss: 1.3184 - cls_loss: 0.8173 - box_loss: 0.0100 - reg_l2_loss: 0.0632 - loss: 1.3816 - learning_rate: 0.0398 - gradient_norm: 1.4297 - val_det_loss: 3.3063 - val_cls_loss: 2.8716 - val_box_loss: 0.0087 - val_reg_l2_loss: 0.0632 - val_loss: 3.3695\n",
            "Epoch 4/50\n",
            "4/4 [==============================] - 119s 30s/step - det_loss: 1.0494 - cls_loss: 0.6379 - box_loss: 0.0082 - reg_l2_loss: 0.0632 - loss: 1.1125 - learning_rate: 0.0395 - gradient_norm: 1.6764 - val_det_loss: 0.9135 - val_cls_loss: 0.5447 - val_box_loss: 0.0074 - val_reg_l2_loss: 0.0632 - val_loss: 0.9767\n",
            "Epoch 5/50\n",
            "4/4 [==============================] - 154s 44s/step - det_loss: 0.8705 - cls_loss: 0.5101 - box_loss: 0.0072 - reg_l2_loss: 0.0632 - loss: 0.9337 - learning_rate: 0.0392 - gradient_norm: 1.3337 - val_det_loss: 0.8883 - val_cls_loss: 0.5500 - val_box_loss: 0.0068 - val_reg_l2_loss: 0.0632 - val_loss: 0.9516\n",
            "Epoch 6/50\n",
            "4/4 [==============================] - 112s 30s/step - det_loss: 0.7238 - cls_loss: 0.4062 - box_loss: 0.0064 - reg_l2_loss: 0.0632 - loss: 0.7870 - learning_rate: 0.0388 - gradient_norm: 0.9858 - val_det_loss: 0.9394 - val_cls_loss: 0.6029 - val_box_loss: 0.0067 - val_reg_l2_loss: 0.0632 - val_loss: 1.0026\n",
            "Epoch 7/50\n",
            "4/4 [==============================] - 108s 29s/step - det_loss: 0.6766 - cls_loss: 0.4029 - box_loss: 0.0055 - reg_l2_loss: 0.0632 - loss: 0.7398 - learning_rate: 0.0383 - gradient_norm: 1.0250 - val_det_loss: 0.8081 - val_cls_loss: 0.5259 - val_box_loss: 0.0056 - val_reg_l2_loss: 0.0632 - val_loss: 0.8714\n",
            "Epoch 8/50\n",
            "4/4 [==============================] - 110s 29s/step - det_loss: 0.5606 - cls_loss: 0.3328 - box_loss: 0.0046 - reg_l2_loss: 0.0632 - loss: 0.6239 - learning_rate: 0.0378 - gradient_norm: 0.7937 - val_det_loss: 0.7900 - val_cls_loss: 0.5034 - val_box_loss: 0.0057 - val_reg_l2_loss: 0.0633 - val_loss: 0.8533\n",
            "Epoch 9/50\n",
            "4/4 [==============================] - 115s 30s/step - det_loss: 0.5411 - cls_loss: 0.3372 - box_loss: 0.0041 - reg_l2_loss: 0.0633 - loss: 0.6044 - learning_rate: 0.0371 - gradient_norm: 0.8987 - val_det_loss: 0.7563 - val_cls_loss: 0.4543 - val_box_loss: 0.0060 - val_reg_l2_loss: 0.0633 - val_loss: 0.8195\n",
            "Epoch 10/50\n",
            "4/4 [==============================] - 132s 37s/step - det_loss: 0.4748 - cls_loss: 0.3000 - box_loss: 0.0035 - reg_l2_loss: 0.0633 - loss: 0.5381 - learning_rate: 0.0364 - gradient_norm: 0.9619 - val_det_loss: 0.7656 - val_cls_loss: 0.4430 - val_box_loss: 0.0065 - val_reg_l2_loss: 0.0633 - val_loss: 0.8289\n",
            "Epoch 11/50\n",
            "4/4 [==============================] - 111s 30s/step - det_loss: 0.4423 - cls_loss: 0.2856 - box_loss: 0.0031 - reg_l2_loss: 0.0633 - loss: 0.5056 - learning_rate: 0.0357 - gradient_norm: 0.8373 - val_det_loss: 0.7782 - val_cls_loss: 0.4212 - val_box_loss: 0.0071 - val_reg_l2_loss: 0.0633 - val_loss: 0.8415\n",
            "Epoch 12/50\n",
            "4/4 [==============================] - 107s 29s/step - det_loss: 0.4376 - cls_loss: 0.2838 - box_loss: 0.0031 - reg_l2_loss: 0.0633 - loss: 0.5009 - learning_rate: 0.0348 - gradient_norm: 0.8522 - val_det_loss: 0.7708 - val_cls_loss: 0.4340 - val_box_loss: 0.0067 - val_reg_l2_loss: 0.0633 - val_loss: 0.8341\n",
            "Epoch 13/50\n",
            "4/4 [==============================] - 109s 29s/step - det_loss: 0.4122 - cls_loss: 0.2745 - box_loss: 0.0028 - reg_l2_loss: 0.0633 - loss: 0.4755 - learning_rate: 0.0340 - gradient_norm: 0.9166 - val_det_loss: 0.7528 - val_cls_loss: 0.4151 - val_box_loss: 0.0068 - val_reg_l2_loss: 0.0633 - val_loss: 0.8161\n",
            "Epoch 14/50\n",
            "4/4 [==============================] - 114s 31s/step - det_loss: 0.3982 - cls_loss: 0.2740 - box_loss: 0.0025 - reg_l2_loss: 0.0633 - loss: 0.4615 - learning_rate: 0.0330 - gradient_norm: 1.0078 - val_det_loss: 0.8453 - val_cls_loss: 0.4660 - val_box_loss: 0.0076 - val_reg_l2_loss: 0.0633 - val_loss: 0.9086\n",
            "Epoch 15/50\n",
            "4/4 [==============================] - 130s 36s/step - det_loss: 0.4034 - cls_loss: 0.2770 - box_loss: 0.0025 - reg_l2_loss: 0.0633 - loss: 0.4667 - learning_rate: 0.0320 - gradient_norm: 1.0153 - val_det_loss: 0.5763 - val_cls_loss: 0.3401 - val_box_loss: 0.0047 - val_reg_l2_loss: 0.0633 - val_loss: 0.6396\n",
            "Epoch 16/50\n",
            "4/4 [==============================] - 108s 29s/step - det_loss: 0.3702 - cls_loss: 0.2606 - box_loss: 0.0022 - reg_l2_loss: 0.0633 - loss: 0.4335 - learning_rate: 0.0310 - gradient_norm: 0.9503 - val_det_loss: 0.6650 - val_cls_loss: 0.3399 - val_box_loss: 0.0065 - val_reg_l2_loss: 0.0633 - val_loss: 0.7283\n",
            "Epoch 17/50\n",
            "4/4 [==============================] - 111s 30s/step - det_loss: 0.3691 - cls_loss: 0.2525 - box_loss: 0.0023 - reg_l2_loss: 0.0633 - loss: 0.4324 - learning_rate: 0.0299 - gradient_norm: 1.1258 - val_det_loss: 0.6274 - val_cls_loss: 0.3387 - val_box_loss: 0.0058 - val_reg_l2_loss: 0.0633 - val_loss: 0.6907\n",
            "Epoch 18/50\n",
            "4/4 [==============================] - 109s 29s/step - det_loss: 0.3470 - cls_loss: 0.2440 - box_loss: 0.0021 - reg_l2_loss: 0.0633 - loss: 0.4103 - learning_rate: 0.0287 - gradient_norm: 0.9753 - val_det_loss: 0.5741 - val_cls_loss: 0.3323 - val_box_loss: 0.0048 - val_reg_l2_loss: 0.0633 - val_loss: 0.6374\n",
            "Epoch 19/50\n",
            "4/4 [==============================] - 115s 31s/step - det_loss: 0.3192 - cls_loss: 0.2265 - box_loss: 0.0019 - reg_l2_loss: 0.0633 - loss: 0.3825 - learning_rate: 0.0276 - gradient_norm: 0.8528 - val_det_loss: 0.6475 - val_cls_loss: 0.3187 - val_box_loss: 0.0066 - val_reg_l2_loss: 0.0633 - val_loss: 0.7108\n",
            "Epoch 20/50\n",
            "4/4 [==============================] - 132s 37s/step - det_loss: 0.2961 - cls_loss: 0.2163 - box_loss: 0.0016 - reg_l2_loss: 0.0633 - loss: 0.3594 - learning_rate: 0.0264 - gradient_norm: 0.8390 - val_det_loss: 0.6200 - val_cls_loss: 0.3119 - val_box_loss: 0.0062 - val_reg_l2_loss: 0.0633 - val_loss: 0.6833\n",
            "Epoch 21/50\n",
            "4/4 [==============================] - 108s 29s/step - det_loss: 0.2972 - cls_loss: 0.2097 - box_loss: 0.0018 - reg_l2_loss: 0.0633 - loss: 0.3605 - learning_rate: 0.0251 - gradient_norm: 0.8927 - val_det_loss: 0.6767 - val_cls_loss: 0.3325 - val_box_loss: 0.0069 - val_reg_l2_loss: 0.0633 - val_loss: 0.7400\n",
            "Epoch 22/50\n",
            "4/4 [==============================] - 109s 29s/step - det_loss: 0.2899 - cls_loss: 0.2014 - box_loss: 0.0018 - reg_l2_loss: 0.0633 - loss: 0.3532 - learning_rate: 0.0239 - gradient_norm: 0.8470 - val_det_loss: 0.6791 - val_cls_loss: 0.3376 - val_box_loss: 0.0068 - val_reg_l2_loss: 0.0633 - val_loss: 0.7424\n",
            "Epoch 23/50\n",
            "4/4 [==============================] - 107s 29s/step - det_loss: 0.2721 - cls_loss: 0.1963 - box_loss: 0.0015 - reg_l2_loss: 0.0633 - loss: 0.3354 - learning_rate: 0.0226 - gradient_norm: 1.0066 - val_det_loss: 0.6464 - val_cls_loss: 0.2773 - val_box_loss: 0.0074 - val_reg_l2_loss: 0.0633 - val_loss: 0.7097\n",
            "Epoch 24/50\n",
            "4/4 [==============================] - 109s 30s/step - det_loss: 0.2594 - cls_loss: 0.1814 - box_loss: 0.0016 - reg_l2_loss: 0.0633 - loss: 0.3227 - learning_rate: 0.0213 - gradient_norm: 1.1475 - val_det_loss: 0.4617 - val_cls_loss: 0.2541 - val_box_loss: 0.0042 - val_reg_l2_loss: 0.0633 - val_loss: 0.5250\n",
            "Epoch 25/50\n",
            "4/4 [==============================] - 129s 36s/step - det_loss: 0.2897 - cls_loss: 0.2041 - box_loss: 0.0017 - reg_l2_loss: 0.0633 - loss: 0.3530 - learning_rate: 0.0201 - gradient_norm: 1.2242 - val_det_loss: 0.5846 - val_cls_loss: 0.3045 - val_box_loss: 0.0056 - val_reg_l2_loss: 0.0633 - val_loss: 0.6479\n",
            "Epoch 26/50\n",
            "4/4 [==============================] - 110s 30s/step - det_loss: 0.2453 - cls_loss: 0.1786 - box_loss: 0.0013 - reg_l2_loss: 0.0633 - loss: 0.3086 - learning_rate: 0.0188 - gradient_norm: 0.9814 - val_det_loss: 0.8491 - val_cls_loss: 0.4251 - val_box_loss: 0.0085 - val_reg_l2_loss: 0.0633 - val_loss: 0.9124\n",
            "Epoch 27/50\n",
            "4/4 [==============================] - 128s 36s/step - det_loss: 0.2661 - cls_loss: 0.1892 - box_loss: 0.0015 - reg_l2_loss: 0.0633 - loss: 0.3294 - learning_rate: 0.0175 - gradient_norm: 1.1091 - val_det_loss: 0.6223 - val_cls_loss: 0.3190 - val_box_loss: 0.0061 - val_reg_l2_loss: 0.0633 - val_loss: 0.6856\n",
            "Epoch 28/50\n",
            "4/4 [==============================] - 110s 29s/step - det_loss: 0.2940 - cls_loss: 0.2187 - box_loss: 0.0015 - reg_l2_loss: 0.0633 - loss: 0.3573 - learning_rate: 0.0162 - gradient_norm: 1.1751 - val_det_loss: 0.6138 - val_cls_loss: 0.3112 - val_box_loss: 0.0061 - val_reg_l2_loss: 0.0633 - val_loss: 0.6771\n",
            "Epoch 29/50\n",
            "4/4 [==============================] - 110s 30s/step - det_loss: 0.2782 - cls_loss: 0.1958 - box_loss: 0.0016 - reg_l2_loss: 0.0633 - loss: 0.3415 - learning_rate: 0.0150 - gradient_norm: 1.0567 - val_det_loss: 0.7284 - val_cls_loss: 0.4589 - val_box_loss: 0.0054 - val_reg_l2_loss: 0.0633 - val_loss: 0.7917\n",
            "Epoch 30/50\n",
            "4/4 [==============================] - 129s 36s/step - det_loss: 0.2612 - cls_loss: 0.1853 - box_loss: 0.0015 - reg_l2_loss: 0.0633 - loss: 0.3245 - learning_rate: 0.0138 - gradient_norm: 1.1824 - val_det_loss: 0.9015 - val_cls_loss: 0.4798 - val_box_loss: 0.0084 - val_reg_l2_loss: 0.0633 - val_loss: 0.9648\n",
            "Epoch 31/50\n",
            "4/4 [==============================] - 110s 29s/step - det_loss: 0.2338 - cls_loss: 0.1676 - box_loss: 0.0013 - reg_l2_loss: 0.0633 - loss: 0.2971 - learning_rate: 0.0126 - gradient_norm: 0.7928 - val_det_loss: 0.6582 - val_cls_loss: 0.3771 - val_box_loss: 0.0056 - val_reg_l2_loss: 0.0633 - val_loss: 0.7215\n",
            "Epoch 32/50\n",
            "4/4 [==============================] - 108s 29s/step - det_loss: 0.2423 - cls_loss: 0.1794 - box_loss: 0.0013 - reg_l2_loss: 0.0633 - loss: 0.3056 - learning_rate: 0.0114 - gradient_norm: 0.8869 - val_det_loss: 0.4688 - val_cls_loss: 0.2484 - val_box_loss: 0.0044 - val_reg_l2_loss: 0.0633 - val_loss: 0.5321\n",
            "Epoch 33/50\n",
            "4/4 [==============================] - 115s 31s/step - det_loss: 0.2313 - cls_loss: 0.1703 - box_loss: 0.0012 - reg_l2_loss: 0.0633 - loss: 0.2946 - learning_rate: 0.0102 - gradient_norm: 0.8597 - val_det_loss: 0.3584 - val_cls_loss: 0.2154 - val_box_loss: 0.0029 - val_reg_l2_loss: 0.0633 - val_loss: 0.4217\n",
            "Epoch 34/50\n",
            "4/4 [==============================] - 107s 29s/step - det_loss: 0.2510 - cls_loss: 0.1764 - box_loss: 0.0015 - reg_l2_loss: 0.0633 - loss: 0.3143 - learning_rate: 0.0091 - gradient_norm: 1.0345 - val_det_loss: 0.3822 - val_cls_loss: 0.2120 - val_box_loss: 0.0034 - val_reg_l2_loss: 0.0633 - val_loss: 0.4455\n",
            "Epoch 35/50\n",
            "4/4 [==============================] - 133s 37s/step - det_loss: 0.2422 - cls_loss: 0.1723 - box_loss: 0.0014 - reg_l2_loss: 0.0633 - loss: 0.3055 - learning_rate: 0.0081 - gradient_norm: 0.9846 - val_det_loss: 0.3283 - val_cls_loss: 0.2044 - val_box_loss: 0.0025 - val_reg_l2_loss: 0.0633 - val_loss: 0.3916\n",
            "Epoch 36/50\n",
            "4/4 [==============================] - 112s 30s/step - det_loss: 0.2312 - cls_loss: 0.1696 - box_loss: 0.0012 - reg_l2_loss: 0.0633 - loss: 0.2945 - learning_rate: 0.0071 - gradient_norm: 0.9283 - val_det_loss: 0.3345 - val_cls_loss: 0.1957 - val_box_loss: 0.0028 - val_reg_l2_loss: 0.0633 - val_loss: 0.3978\n",
            "Epoch 37/50\n",
            "4/4 [==============================] - 108s 29s/step - det_loss: 0.2161 - cls_loss: 0.1573 - box_loss: 0.0012 - reg_l2_loss: 0.0633 - loss: 0.2794 - learning_rate: 0.0061 - gradient_norm: 0.8357 - val_det_loss: 0.3231 - val_cls_loss: 0.1906 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.0633 - val_loss: 0.3864\n",
            "Epoch 38/50\n",
            "4/4 [==============================] - 115s 31s/step - det_loss: 0.2165 - cls_loss: 0.1567 - box_loss: 0.0012 - reg_l2_loss: 0.0633 - loss: 0.2798 - learning_rate: 0.0052 - gradient_norm: 0.8315 - val_det_loss: 0.2860 - val_cls_loss: 0.1833 - val_box_loss: 0.0021 - val_reg_l2_loss: 0.0633 - val_loss: 0.3493\n",
            "Epoch 39/50\n",
            "4/4 [==============================] - 110s 30s/step - det_loss: 0.2337 - cls_loss: 0.1731 - box_loss: 0.0012 - reg_l2_loss: 0.0633 - loss: 0.2970 - learning_rate: 0.0044 - gradient_norm: 0.9459 - val_det_loss: 0.2989 - val_cls_loss: 0.1786 - val_box_loss: 0.0024 - val_reg_l2_loss: 0.0633 - val_loss: 0.3622\n",
            "Epoch 40/50\n",
            "4/4 [==============================] - 135s 38s/step - det_loss: 0.2175 - cls_loss: 0.1570 - box_loss: 0.0012 - reg_l2_loss: 0.0633 - loss: 0.2808 - learning_rate: 0.0036 - gradient_norm: 0.9366 - val_det_loss: 0.2911 - val_cls_loss: 0.1746 - val_box_loss: 0.0023 - val_reg_l2_loss: 0.0633 - val_loss: 0.3544\n",
            "Epoch 41/50\n",
            "4/4 [==============================] - 112s 30s/step - det_loss: 0.2141 - cls_loss: 0.1552 - box_loss: 0.0012 - reg_l2_loss: 0.0633 - loss: 0.2774 - learning_rate: 0.0029 - gradient_norm: 0.7366 - val_det_loss: 0.2624 - val_cls_loss: 0.1705 - val_box_loss: 0.0018 - val_reg_l2_loss: 0.0633 - val_loss: 0.3257\n",
            "Epoch 42/50\n",
            "4/4 [==============================] - 109s 29s/step - det_loss: 0.2231 - cls_loss: 0.1634 - box_loss: 0.0012 - reg_l2_loss: 0.0633 - loss: 0.2864 - learning_rate: 0.0023 - gradient_norm: 1.0450 - val_det_loss: 0.2526 - val_cls_loss: 0.1676 - val_box_loss: 0.0017 - val_reg_l2_loss: 0.0633 - val_loss: 0.3159\n",
            "Epoch 43/50\n",
            "4/4 [==============================] - 140s 38s/step - det_loss: 0.2152 - cls_loss: 0.1551 - box_loss: 0.0012 - reg_l2_loss: 0.0633 - loss: 0.2785 - learning_rate: 0.0017 - gradient_norm: 0.6855 - val_det_loss: 0.2492 - val_cls_loss: 0.1638 - val_box_loss: 0.0017 - val_reg_l2_loss: 0.0633 - val_loss: 0.3125\n",
            "Epoch 44/50\n",
            "4/4 [==============================] - 116s 31s/step - det_loss: 0.2133 - cls_loss: 0.1545 - box_loss: 0.0012 - reg_l2_loss: 0.0633 - loss: 0.2766 - learning_rate: 0.0013 - gradient_norm: 0.6133 - val_det_loss: 0.2450 - val_cls_loss: 0.1607 - val_box_loss: 0.0017 - val_reg_l2_loss: 0.0633 - val_loss: 0.3083\n",
            "Epoch 45/50\n",
            "4/4 [==============================] - 133s 37s/step - det_loss: 0.2053 - cls_loss: 0.1483 - box_loss: 0.0011 - reg_l2_loss: 0.0633 - loss: 0.2686 - learning_rate: 8.4833e-04 - gradient_norm: 0.6646 - val_det_loss: 0.2379 - val_cls_loss: 0.1580 - val_box_loss: 0.0016 - val_reg_l2_loss: 0.0633 - val_loss: 0.3012\n",
            "Epoch 46/50\n",
            "4/4 [==============================] - 110s 29s/step - det_loss: 0.2124 - cls_loss: 0.1531 - box_loss: 0.0012 - reg_l2_loss: 0.0633 - loss: 0.2757 - learning_rate: 5.1921e-04 - gradient_norm: 0.6545 - val_det_loss: 0.2315 - val_cls_loss: 0.1553 - val_box_loss: 0.0015 - val_reg_l2_loss: 0.0633 - val_loss: 0.2948\n",
            "Epoch 47/50\n",
            "4/4 [==============================] - 110s 30s/step - det_loss: 0.2158 - cls_loss: 0.1577 - box_loss: 0.0012 - reg_l2_loss: 0.0633 - loss: 0.2791 - learning_rate: 2.7014e-04 - gradient_norm: 0.7459 - val_det_loss: 0.2268 - val_cls_loss: 0.1533 - val_box_loss: 0.0015 - val_reg_l2_loss: 0.0633 - val_loss: 0.2901\n",
            "Epoch 48/50\n",
            "4/4 [==============================] - 119s 32s/step - det_loss: 0.2291 - cls_loss: 0.1607 - box_loss: 0.0014 - reg_l2_loss: 0.0633 - loss: 0.2924 - learning_rate: 1.0215e-04 - gradient_norm: 0.8432 - val_det_loss: 0.2229 - val_cls_loss: 0.1514 - val_box_loss: 0.0014 - val_reg_l2_loss: 0.0633 - val_loss: 0.2862\n",
            "Epoch 49/50\n",
            "4/4 [==============================] - 109s 29s/step - det_loss: 0.2334 - cls_loss: 0.1686 - box_loss: 0.0013 - reg_l2_loss: 0.0633 - loss: 0.2967 - learning_rate: 1.5925e-05 - gradient_norm: 0.9489 - val_det_loss: 0.2194 - val_cls_loss: 0.1498 - val_box_loss: 0.0014 - val_reg_l2_loss: 0.0633 - val_loss: 0.2827\n",
            "Epoch 50/50\n",
            "4/4 [==============================] - 132s 37s/step - det_loss: 0.2233 - cls_loss: 0.1578 - box_loss: 0.0013 - reg_l2_loss: 0.0633 - loss: 0.2866 - learning_rate: 1.1816e-05 - gradient_norm: 0.7648 - val_det_loss: 0.2158 - val_cls_loss: 0.1481 - val_box_loss: 0.0014 - val_reg_l2_loss: 0.0633 - val_loss: 0.2791\n"
          ]
        }
      ],
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=32, train_whole_model=True, epochs=50, validation_data=val_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.export(\"exported_manually\") #this created (probably quantized) model.tflite"
      ],
      "metadata": {
        "id": "k9Hkcp_QxRJX"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB4hKeerMmh4"
      },
      "source": [
        "### Step 4. Evaluate the model with the validation data.\n",
        "\n",
        "After training the object detection model using the images in the training dataset, use the 10 images in the validation dataset to evaluate how the model performs against new data it has never seen before.\n",
        "\n",
        "As the default batch size is 64, it will take 1 step to go through the 10 images in the validation dataset.\n",
        "\n",
        "The evaluation metrics are same as [COCO](https://cocodataset.org/#detection-eval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUqEpcYwAg8L",
        "outputId": "748b87f5-1abb-4034-aea0-4d146bb7c323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 42s 10s/step\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AP': 0.6839176,\n",
              " 'AP50': 0.921303,\n",
              " 'AP75': 0.85154,\n",
              " 'APs': -1.0,\n",
              " 'APm': 0.3431458,\n",
              " 'APl': 0.7370058,\n",
              " 'ARmax1': 0.46440074,\n",
              " 'ARmax10': 0.75621015,\n",
              " 'ARmax100': 0.761909,\n",
              " 'ARs': -1.0,\n",
              " 'ARm': 0.43819445,\n",
              " 'ARl': 0.8124716,\n",
              " 'AP_/a_yolk': 0.75187564,\n",
              " 'AP_/b_yolk': 0.6159595,\n",
              " 'AP_/c_yolk': -1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "model.evaluate(val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NARVYk9rGLIl"
      },
      "source": [
        "### Step 5: Export as a TensorFlow Lite model.\n",
        "\n",
        "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is [full integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant). This allows the TensorFlow Lite model to be smaller, run faster on Raspberry Pi CPU and also compatible with the Google Coral EdgeTPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_u3eFxoBAiqE"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', tflite_filename='eggs.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZcBmEigOCO3"
      },
      "source": [
        "### Step 6:  Evaluate the TensorFlow Lite model.\n",
        "\n",
        "Several factors can affect the model accuracy when exporting to TFLite:\n",
        "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop.\n",
        "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
        "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
        "\n",
        "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbl8z9_wBPlr",
        "outputId": "197f303d-974b-4a99-df69-8285e6bffff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "154/154 [==============================] - 554s 4s/step\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AP': 0.6675156,\n",
              " 'AP50': 0.90123034,\n",
              " 'AP75': 0.838558,\n",
              " 'APs': -1.0,\n",
              " 'APm': 0.2899329,\n",
              " 'APl': 0.72653127,\n",
              " 'ARmax1': 0.45670432,\n",
              " 'ARmax10': 0.70733964,\n",
              " 'ARmax100': 0.70733964,\n",
              " 'ARs': -1.0,\n",
              " 'ARm': 0.3251984,\n",
              " 'ARl': 0.766316,\n",
              " 'AP_/a_yolk': 0.7396045,\n",
              " 'AP_/b_yolk': 0.59542674,\n",
              " 'AP_/c_yolk': -1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "model.evaluate_tflite('eggs.tflite', val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnqktl45PZRy"
      },
      "source": [
        "## Test the Android figurine detection model\n",
        "\n",
        "After training the model, let's test it with an image that the model hasn't seen before to get a sense of how good the model is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "cellView": "form",
        "id": "9ZsLQtJ1AlW_"
      },
      "outputs": [],
      "source": [
        "#@title Load the trained TFLite model and define some visualization functions\n",
        "\n",
        "#@markdown This code comes from the TFLite Object Detection [Raspberry Pi sample](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/raspberry_pi).\n",
        "\n",
        "import platform\n",
        "from typing import List, NamedTuple\n",
        "import json\n",
        "\n",
        "import cv2\n",
        "\n",
        "Interpreter = tf.lite.Interpreter\n",
        "load_delegate = tf.lite.experimental.load_delegate\n",
        "\n",
        "# pylint: enable=g-import-not-at-top\n",
        "\n",
        "\n",
        "class ObjectDetectorOptions(NamedTuple):\n",
        "  \"\"\"A config to initialize an object detector.\"\"\"\n",
        "\n",
        "  enable_edgetpu: bool = False\n",
        "  \"\"\"Enable the model to run on EdgeTPU.\"\"\"\n",
        "\n",
        "  label_allow_list: List[str] = None\n",
        "  \"\"\"The optional allow list of labels.\"\"\"\n",
        "\n",
        "  label_deny_list: List[str] = None\n",
        "  \"\"\"The optional deny list of labels.\"\"\"\n",
        "\n",
        "  max_results: int = -1\n",
        "  \"\"\"The maximum number of top-scored detection results to return.\"\"\"\n",
        "\n",
        "  num_threads: int = 1\n",
        "  \"\"\"The number of CPU threads to be used.\"\"\"\n",
        "\n",
        "  score_threshold: float = 0.0\n",
        "  \"\"\"The score threshold of detection results to return.\"\"\"\n",
        "\n",
        "\n",
        "class Rect(NamedTuple):\n",
        "  \"\"\"A rectangle in 2D space.\"\"\"\n",
        "  left: float\n",
        "  top: float\n",
        "  right: float\n",
        "  bottom: float\n",
        "\n",
        "\n",
        "class Category(NamedTuple):\n",
        "  \"\"\"A result of a classification task.\"\"\"\n",
        "  label: str\n",
        "  score: float\n",
        "  index: int\n",
        "\n",
        "\n",
        "class Detection(NamedTuple):\n",
        "  \"\"\"A detected object as the result of an ObjectDetector.\"\"\"\n",
        "  bounding_box: Rect\n",
        "  categories: List[Category]\n",
        "\n",
        "\n",
        "def edgetpu_lib_name():\n",
        "  \"\"\"Returns the library name of EdgeTPU in the current platform.\"\"\"\n",
        "  return {\n",
        "      'Darwin': 'libedgetpu.1.dylib',\n",
        "      'Linux': 'libedgetpu.so.1',\n",
        "      'Windows': 'edgetpu.dll',\n",
        "  }.get(platform.system(), None)\n",
        "\n",
        "\n",
        "class ObjectDetector:\n",
        "  \"\"\"A wrapper class for a TFLite object detection model.\"\"\"\n",
        "\n",
        "  _OUTPUT_LOCATION_NAME = 'location'\n",
        "  _OUTPUT_CATEGORY_NAME = 'category'\n",
        "  _OUTPUT_SCORE_NAME = 'score'\n",
        "  _OUTPUT_NUMBER_NAME = 'number of detections'\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      model_path: str,\n",
        "      options: ObjectDetectorOptions = ObjectDetectorOptions()\n",
        "  ) -> None:\n",
        "    \"\"\"Initialize a TFLite object detection model.\n",
        "    Args:\n",
        "        model_path: Path to the TFLite model.\n",
        "        options: The config to initialize an object detector. (Optional)\n",
        "    Raises:\n",
        "        ValueError: If the TFLite model is invalid.\n",
        "        OSError: If the current OS isn't supported by EdgeTPU.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load metadata from model.\n",
        "    displayer = metadata.MetadataDisplayer.with_model_file(model_path)\n",
        "\n",
        "    # Save model metadata for preprocessing later.\n",
        "    model_metadata = json.loads(displayer.get_metadata_json())\n",
        "    process_units = model_metadata['subgraph_metadata'][0]['input_tensor_metadata'][0]['process_units']\n",
        "    mean = 0.0\n",
        "    std = 1.0\n",
        "    for option in process_units:\n",
        "      if option['options_type'] == 'NormalizationOptions':\n",
        "        mean = option['options']['mean'][0]\n",
        "        std = option['options']['std'][0]\n",
        "    self._mean = mean\n",
        "    self._std = std\n",
        "\n",
        "    # Load label list from metadata.\n",
        "    file_name = displayer.get_packed_associated_file_list()[0]\n",
        "    label_map_file = displayer.get_associated_file_buffer(file_name).decode()\n",
        "    label_list = list(filter(lambda x: len(x) > 0, label_map_file.splitlines()))\n",
        "    self._label_list = label_list\n",
        "\n",
        "    # Initialize TFLite model.\n",
        "    if options.enable_edgetpu:\n",
        "      if edgetpu_lib_name() is None:\n",
        "        raise OSError(\"The current OS isn't supported by Coral EdgeTPU.\")\n",
        "      interpreter = Interpreter(\n",
        "          model_path=model_path,\n",
        "          experimental_delegates=[load_delegate(edgetpu_lib_name())],\n",
        "          num_threads=options.num_threads)\n",
        "    else:\n",
        "      interpreter = Interpreter(\n",
        "          model_path=model_path, num_threads=options.num_threads)\n",
        "\n",
        "    interpreter.allocate_tensors()\n",
        "    input_detail = interpreter.get_input_details()[0]\n",
        "\n",
        "    # From TensorFlow 2.6, the order of the outputs become undefined.\n",
        "    # Therefore we need to sort the tensor indices of TFLite outputs and to know\n",
        "    # exactly the meaning of each output tensor. For example, if\n",
        "    # output indices are [601, 599, 598, 600], tensor names and indices aligned\n",
        "    # are:\n",
        "    #   - location: 598\n",
        "    #   - category: 599\n",
        "    #   - score: 600\n",
        "    #   - detection_count: 601\n",
        "    # because of the op's ports of TFLITE_DETECTION_POST_PROCESS\n",
        "    # (https://github.com/tensorflow/tensorflow/blob/a4fe268ea084e7d323133ed7b986e0ae259a2bc7/tensorflow/lite/kernels/detection_postprocess.cc#L47-L50).\n",
        "    sorted_output_indices = sorted(\n",
        "        [output['index'] for output in interpreter.get_output_details()])\n",
        "    self._output_indices = {\n",
        "        self._OUTPUT_LOCATION_NAME: sorted_output_indices[0],\n",
        "        self._OUTPUT_CATEGORY_NAME: sorted_output_indices[1],\n",
        "        self._OUTPUT_SCORE_NAME: sorted_output_indices[2],\n",
        "        self._OUTPUT_NUMBER_NAME: sorted_output_indices[3],\n",
        "    }\n",
        "\n",
        "    self._input_size = input_detail['shape'][2], input_detail['shape'][1]\n",
        "    self._is_quantized_input = input_detail['dtype'] == np.uint8\n",
        "    self._interpreter = interpreter\n",
        "    self._options = options\n",
        "\n",
        "  def detect(self, input_image: np.ndarray) -> List[Detection]:\n",
        "    \"\"\"Run detection on an input image.\n",
        "    Args:\n",
        "        input_image: A [height, width, 3] RGB image. Note that height and width\n",
        "          can be anything since the image will be immediately resized according\n",
        "          to the needs of the model within this function.\n",
        "    Returns:\n",
        "        A Person instance.\n",
        "    \"\"\"\n",
        "    image_height, image_width, _ = input_image.shape\n",
        "\n",
        "    input_tensor = self._preprocess(input_image)\n",
        "\n",
        "    self._set_input_tensor(input_tensor)\n",
        "    self._interpreter.invoke()\n",
        "\n",
        "    # Get all output details\n",
        "    boxes = self._get_output_tensor(self._OUTPUT_LOCATION_NAME)\n",
        "    classes = self._get_output_tensor(self._OUTPUT_CATEGORY_NAME)\n",
        "    scores = self._get_output_tensor(self._OUTPUT_SCORE_NAME)\n",
        "    count = int(self._get_output_tensor(self._OUTPUT_NUMBER_NAME))\n",
        "\n",
        "    return self._postprocess(boxes, classes, scores, count, image_width,\n",
        "                             image_height)\n",
        "\n",
        "  def _preprocess(self, input_image: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Preprocess the input image as required by the TFLite model.\"\"\"\n",
        "\n",
        "    # Resize the input\n",
        "    input_tensor = cv2.resize(input_image, self._input_size)\n",
        "\n",
        "    # Normalize the input if it's a float model (aka. not quantized)\n",
        "    if not self._is_quantized_input:\n",
        "      input_tensor = (np.float32(input_tensor) - self._mean) / self._std\n",
        "\n",
        "    # Add batch dimension\n",
        "    input_tensor = np.expand_dims(input_tensor, axis=0)\n",
        "\n",
        "    return input_tensor\n",
        "\n",
        "  def _set_input_tensor(self, image):\n",
        "    \"\"\"Sets the input tensor.\"\"\"\n",
        "    tensor_index = self._interpreter.get_input_details()[0]['index']\n",
        "    input_tensor = self._interpreter.tensor(tensor_index)()[0]\n",
        "    input_tensor[:, :] = image\n",
        "\n",
        "  def _get_output_tensor(self, name):\n",
        "    \"\"\"Returns the output tensor at the given index.\"\"\"\n",
        "    output_index = self._output_indices[name]\n",
        "    tensor = np.squeeze(self._interpreter.get_tensor(output_index))\n",
        "    return tensor\n",
        "\n",
        "  def _postprocess(self, boxes: np.ndarray, classes: np.ndarray,\n",
        "                   scores: np.ndarray, count: int, image_width: int,\n",
        "                   image_height: int) -> List[Detection]:\n",
        "    \"\"\"Post-process the output of TFLite model into a list of Detection objects.\n",
        "    Args:\n",
        "        boxes: Bounding boxes of detected objects from the TFLite model.\n",
        "        classes: Class index of the detected objects from the TFLite model.\n",
        "        scores: Confidence scores of the detected objects from the TFLite model.\n",
        "        count: Number of detected objects from the TFLite model.\n",
        "        image_width: Width of the input image.\n",
        "        image_height: Height of the input image.\n",
        "    Returns:\n",
        "        A list of Detection objects detected by the TFLite model.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Parse the model output into a list of Detection entities.\n",
        "    for i in range(count):\n",
        "      if scores[i] >= self._options.score_threshold:\n",
        "        y_min, x_min, y_max, x_max = boxes[i]\n",
        "        bounding_box = Rect(\n",
        "            top=int(y_min * image_height),\n",
        "            left=int(x_min * image_width),\n",
        "            bottom=int(y_max * image_height),\n",
        "            right=int(x_max * image_width))\n",
        "        class_id = int(classes[i])\n",
        "        category = Category(\n",
        "            score=scores[i],\n",
        "            label=self._label_list[class_id],  # 0 is reserved for background\n",
        "            index=class_id)\n",
        "        result = Detection(bounding_box=bounding_box, categories=[category])\n",
        "        results.append(result)\n",
        "\n",
        "    # Sort detection results by score ascending\n",
        "    sorted_results = sorted(\n",
        "        results,\n",
        "        key=lambda detection: detection.categories[0].score,\n",
        "        reverse=True)\n",
        "\n",
        "    # Filter out detections in deny list\n",
        "    filtered_results = sorted_results\n",
        "    if self._options.label_deny_list is not None:\n",
        "      filtered_results = list(\n",
        "          filter(\n",
        "              lambda detection: detection.categories[0].label not in self.\n",
        "              _options.label_deny_list, filtered_results))\n",
        "\n",
        "    # Keep only detections in allow list\n",
        "    if self._options.label_allow_list is not None:\n",
        "      filtered_results = list(\n",
        "          filter(\n",
        "              lambda detection: detection.categories[0].label in self._options.\n",
        "              label_allow_list, filtered_results))\n",
        "\n",
        "    # Only return maximum of max_results detection.\n",
        "    if self._options.max_results > 0:\n",
        "      result_count = min(len(filtered_results), self._options.max_results)\n",
        "      filtered_results = filtered_results[:result_count]\n",
        "\n",
        "    return filtered_results\n",
        "\n",
        "\n",
        "_MARGIN = 10  # pixels\n",
        "_ROW_SIZE = 10  # pixels\n",
        "_FONT_SIZE = 1\n",
        "_FONT_THICKNESS = 1\n",
        "_TEXT_COLOR = (0, 0, 255)  # red\n",
        "\n",
        "\n",
        "def visualize(\n",
        "    image: np.ndarray,\n",
        "    detections: List[Detection],\n",
        ") -> np.ndarray:\n",
        "  \"\"\"Draws bounding boxes on the input image and return it.\n",
        "  Args:\n",
        "    image: The input RGB image.\n",
        "    detections: The list of all \"Detection\" entities to be visualize.\n",
        "  Returns:\n",
        "    Image with bounding boxes.\n",
        "  \"\"\"\n",
        "  for detection in detections:\n",
        "    # Draw bounding_box\n",
        "    start_point = detection.bounding_box.left, detection.bounding_box.top\n",
        "    end_point = detection.bounding_box.right, detection.bounding_box.bottom\n",
        "    cv2.rectangle(image, start_point, end_point, _TEXT_COLOR, 3)\n",
        "\n",
        "    # Draw label and score\n",
        "    category = detection.categories[0]\n",
        "    class_name = category.label\n",
        "    probability = round(category.score, 2)\n",
        "    result_text = class_name + ' (' + str(probability) + ')'\n",
        "    text_location = (_MARGIN + detection.bounding_box.left,\n",
        "                     _MARGIN + _ROW_SIZE + detection.bounding_box.top)\n",
        "    cv2.putText(image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
        "                _FONT_SIZE, _TEXT_COLOR, _FONT_THICKNESS)\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "cellView": "form",
        "id": "1t1z2fKlAoB0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "6f28e32e-44dc-409f-ae11-283dac89c9f0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-6d1d628bb1f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mscore_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDETECTION_THRESHOLD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjectDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTFLITE_MODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Run object detection estimation using the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-dc17e2eccaaa>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, options)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Load metadata from model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdisplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetadataDisplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_model_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# Save model metadata for preprocessing later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_lite_support/metadata/python/metadata.py\u001b[0m in \u001b[0;36mwith_model_file\u001b[0;34m(cls, model_file)\u001b[0m\n\u001b[1;32m    715\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m     \"\"\"\n\u001b[0;32m--> 717\u001b[0;31m     \u001b[0m_assert_file_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_model_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_lite_support/metadata/python/metadata.py\u001b[0m in \u001b[0;36m_assert_file_exist\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    818\u001b[0m   \u001b[0;34m\"\"\"Checks if a file exists.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_exists_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File, '{0}', does not exist.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: File, 'android.tflite', does not exist."
          ]
        }
      ],
      "source": [
        "#@title Run object detection and show the detection results\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "INPUT_IMAGE_URL = \"http://download.tensorflow.org/example_images/android_figurine.jpg\" #@param {type:\"string\"}\n",
        "DETECTION_THRESHOLD = 0.5 #@param {type:\"number\"}\n",
        "TFLITE_MODEL_PATH = \"android.tflite\" #@param {type:\"string\"}\n",
        "\n",
        "TEMP_FILE = '/tmp/image.png'\n",
        "\n",
        "!wget -q -O $TEMP_FILE $INPUT_IMAGE_URL\n",
        "image = Image.open(TEMP_FILE).convert('RGB')\n",
        "image.thumbnail((512, 512), Image.ANTIALIAS)\n",
        "image_np = np.asarray(image)\n",
        "\n",
        "# Load the TFLite model\n",
        "options = ObjectDetectorOptions(\n",
        "      num_threads=4,\n",
        "      score_threshold=DETECTION_THRESHOLD,\n",
        ")\n",
        "detector = ObjectDetector(model_path=TFLITE_MODEL_PATH, options=options)\n",
        "\n",
        "# Run object detection estimation using the model.\n",
        "detections = detector.detect(image_np)\n",
        "\n",
        "# Draw keypoints and edges on input image\n",
        "image_np = visualize(image_np, detections)\n",
        "\n",
        "# Show the detection result\n",
        "Image.fromarray(image_np)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert 0==1 # Stop execution, because I don't have EdgeTPU Coral device at hand"
      ],
      "metadata": {
        "id": "INskgRkZb4aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWP3fEPaGNvd"
      },
      "source": [
        "## Compile the model for EdgeTPU\n",
        "\n",
        "Finally, we'll compile the model using `edgetpu_compiler` so that the model can run on [Google Coral EdgeTPU](https://coral.ai/).\n",
        "\n",
        "We start with installing the EdgeTPU compiler on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK6AN1xVAsCb"
      },
      "outputs": [],
      "source": [
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install edgetpu-compiler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIGSdzXkEzrj"
      },
      "source": [
        "**Note:** When training the model using a custom dataset, beware that if your dataset includes more than 20 classes, you'll probably have slower inference speeds compared to if you have fewer classes. This is due to an aspect of the EfficientDet architecture in which a certain layer cannot compile for the Edge TPU when it carries more than 20 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzF6u0FZTAjF"
      },
      "source": [
        "Before compiling the `.tflite` file for the Edge TPU, it's important to consider whether your model will fit into the Edge TPU memory. \n",
        "\n",
        "The Edge TPU has approximately 8 MB of SRAM for [caching model paramaters](https://coral.ai/docs/edgetpu/compiler/#parameter-data-caching), so any model close to or over 8 MB will not fit onto the Edge TPU memory. That means the inference times are longer, because some model parameters must be fetched from the host system memory.\n",
        "\n",
        "One way to elimiate the extra latency is to use [model pipelining](https://coral.ai/docs/edgetpu/pipeline/), which splits the model into segments that can run on separate Edge TPUs in series. This can significantly reduce the latency for big models.\n",
        "\n",
        "The following table provides recommendations for the number of Edge TPUs to use with each EfficientDet-Lite model.\n",
        "\n",
        "| Model architecture | Minimum TPUs | Recommended TPUs\n",
        "|--------------------|-------|-------|\n",
        "| EfficientDet-Lite0 | 1     | 1     |\n",
        "| EfficientDet-Lite1 | 1     | 1     |\n",
        "| EfficientDet-Lite2 | 1     | 2     |\n",
        "| EfficientDet-Lite3 | 2     | 2     |\n",
        "| EfficientDet-Lite4 | 2     | 3     |\n",
        "\n",
        "If you need extra Edge TPUs for your model, then update `NUMBER_OF_TPUS` here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyptUjakAwzz"
      },
      "outputs": [],
      "source": [
        "NUMBER_OF_TPUS = 1\n",
        "\n",
        "!edgetpu_compiler android.tflite --num_segments=$NUMBER_OF_TPUS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJYXucYWTGqZ"
      },
      "source": [
        "Finally, we'll copy the metadata, including the label file, from the original TensorFlow Lite model to the EdgeTPU model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LY1WrgMJBFd"
      },
      "outputs": [],
      "source": [
        "populator_dst = metadata.MetadataPopulator.with_model_file('android_edgetpu.tflite')\n",
        "\n",
        "with open('android.tflite', 'rb') as f:\n",
        "  populator_dst.load_metadata_and_associated_files(f.read())\n",
        "\n",
        "populator_dst.populate()\n",
        "updated_model_buf = populator_dst.get_model_buffer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdRihInCJ3ie"
      },
      "outputs": [],
      "source": [
        "# Download the TFLite model compiled for EdgeTPU to your local computer.\n",
        "from google.colab import files\n",
        "files.download('android_edgetpu.tflite')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}