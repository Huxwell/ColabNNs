{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1CvFx7AXZEOMMQSwMggEKbqKJU4_ogxUW",
      "authorship_tag": "ABX9TyM8M9RgmcfWLG0RxZj1COke",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Huxwell/ColabNNs/blob/main/cats_n_dogs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0XYxT3MBckL"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import random # do random.seed(13) before every shuffle. order of shuffle() execution changes results order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -rv runs"
      ],
      "metadata": {
        "id": "ZLnT2cnD3Gp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- I have no labels for test here, so I am dropping 'test.zip' related code. I can split train into train, val, test; in fact I don't want to have a lot of examples for train set.\n",
        "- The sets are almost balanced, accuracy is ok here"
      ],
      "metadata": {
        "id": "gMdiSs9-eB7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "LGsOW96X9y6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(13)\n",
        "if device =='cuda':\n",
        "    torch.cuda.manual_seed_all(13)"
      ],
      "metadata": {
        "id": "JZRBEouz9xuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = '/content/drive/MyDrive/cats_n_dogs_unsure/dogs-vs-cats-redux-kernels-edition'\n",
        "os.listdir(dataset_dir)"
      ],
      "metadata": {
        "id": "Kw2dMvob99Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('local_data/', exist_ok=True)\n",
        "train_dir = 'local_data/train'"
      ],
      "metadata": {
        "id": "N8slOqO8-IUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile(os.path.join(dataset_dir, 'train.zip')) as train_zip:\n",
        "    train_zip.extractall('local_data')"
      ],
      "metadata": {
        "id": "JytonT65-L6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(train_dir)[:5]"
      ],
      "metadata": {
        "id": "RzvXLys8CNY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cats_list = sorted(glob.glob(os.path.join(train_dir,'cat*.jpg')))\n",
        "dogs_list = sorted(glob.glob(os.path.join(train_dir,'dog*.jpg')))\n",
        "print(len(cats_list))\n",
        "print(len(dogs_list))\n",
        "random.seed(13)\n",
        "random.shuffle(cats_list)\n",
        "random.seed(13) #multiple seed() executions are on purpose.\n",
        "random.shuffle(dogs_list)\n",
        "print(cats_list[:3])\n",
        "print(dogs_list[:3])\n",
        "# some sanity check to make sure no uncontrolled randomness beyond this point\n",
        "assert cats_list[2] == 'local_data/train/cat.801.jpg'\n",
        "assert dogs_list[2] == 'local_data/train/dog.801.jpg'\n",
        "assert len(cats_list) == len(dogs_list)\n"
      ],
      "metadata": {
        "id": "FCmaWu9nfx9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_train_list = cats_list[:7500] + dogs_list[:7500]\n",
        "subset_train_list = cats_list[:250] + dogs_list[:250]\n",
        "val_list = cats_list[7500:10000] + dogs_list[7500:10000]\n",
        "test_list = cats_list[10000:] + dogs_list[10000:]\n",
        "print(len(full_train_list), len(val_list), len(test_list))\n",
        "\n",
        "random.seed(13)\n",
        "random.shuffle(full_train_list)\n",
        "random.seed(13)\n",
        "random.shuffle(subset_train_list)\n",
        "random.seed(13)\n",
        "random.shuffle(val_list)\n",
        "random.seed(13)\n",
        "random.shuffle(test_list)\n",
        "print(full_train_list[:9])\n",
        "print(subset_train_list[:9])\n",
        "print(val_list[:9])\n",
        "print(test_list[:9])\n",
        "assert full_train_list[4] == 'local_data/train/cat.1612.jpg'\n",
        "assert subset_train_list[4] == 'local_data/train/cat.1787.jpg'\n",
        "assert val_list[4] == 'local_data/train/dog.12023.jpg'\n",
        "assert test_list[4] == 'local_data/train/dog.6485.jpg'"
      ],
      "metadata": {
        "id": "ADT-iYGCiOMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list = subset_train_list # 5 batches"
      ],
      "metadata": {
        "id": "R_iriJP1lzl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(13131313)\n",
        "random_idx = np.random.randint(1,len(train_list),size=10)\n",
        "print(random_idx)\n",
        "fig = plt.figure(figsize=(20., 10.))\n",
        "\n",
        "for i, img_idx in enumerate(random_idx):\n",
        "    ax = fig.add_subplot(2,5,i+1)\n",
        "    img = Image.open(train_list[img_idx])\n",
        "    plt.imshow(img)\n",
        "    i+=1\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jgRVni0iCVkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_list[0].split('/')[-1].split('.')[0]"
      ],
      "metadata": {
        "id": "jSk64R7gDGsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "MpFt-8n8DgfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms =  transforms.Compose([\n",
        "  transforms.Resize((224, 224)),\n",
        "  transforms.RandomResizedCrop(224),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "  transforms.Resize((224, 224)),\n",
        "  transforms.RandomResizedCrop(224),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([   \n",
        "  transforms.Resize((224, 224)),\n",
        "  transforms.RandomResizedCrop(224),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "AM5LfbFcDf_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,file_list,transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "        \n",
        "    #dataset length\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.file_list)\n",
        "        return self.filelength\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        img = Image.open(img_path)\n",
        "        img_transformed = self.transform(img)\n",
        "        \n",
        "        label = img_path.split('/')[-1].split('.')[0]\n",
        "        if label == 'dog':\n",
        "            label=1\n",
        "        elif label == 'cat':\n",
        "            label=0\n",
        "            \n",
        "        return img_transformed,label"
      ],
      "metadata": {
        "id": "u-TsOoMoDsur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dataset(train_list, transform=train_transforms)\n",
        "val_data = dataset(val_list, transform=test_transforms)\n",
        "test_data = dataset(val_list, transform=test_transforms)"
      ],
      "metadata": {
        "id": "936yLEVWDyc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100 # we will use mini-batch method"
      ],
      "metadata": {
        "id": "8koteIep9wIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size=batch_size, shuffle=False )\n",
        "val_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "cZ2krjGVDzuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data), len(train_loader), len(test_loader))"
      ],
      "metadata": {
        "id": "QWizETdhD1h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(1 for filename in train_data.file_list if 'cat' in filename))\n",
        "print(sum(1 for filename in train_data.file_list if 'dog' in filename))\n",
        "print(sum(1 for filename in val_data.file_list if 'cat' in filename))\n",
        "print(sum(1 for filename in val_data.file_list if 'dog' in filename))\n",
        "print(sum(1 for filename in test_data.file_list if 'cat' in filename))\n",
        "print(sum(1 for filename in test_data.file_list if 'dog' in filename))"
      ],
      "metadata": {
        "id": "mAfRCxgVbseH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check our images shape\n",
        "train_data[0][0].shape"
      ],
      "metadata": {
        "id": "8v2wAJjCD3Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)"
      ],
      "metadata": {
        "id": "81GkYD0_D6ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "model.train()"
      ],
      "metadata": {
        "id": "VDZjG9EbES1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(params = model.parameters(),lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "WE4uuPYWEbsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv -v runs/ temp_runs_backup/"
      ],
      "metadata": {
        "id": "EY-1xfJ7yR9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter()\n",
        "epochs = 10 #10\n",
        "min_val_loss = float('inf')\n",
        "epochs_since_min_loss = 0\n",
        "patience = 7\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\n\",\"=\"*30,\"\\n\")\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    \n",
        "    with tqdm(train_loader, unit=\"iteration\") as tepoch:\n",
        "      \n",
        "      for data, label in tepoch:\n",
        "          tepoch.set_description(f\"Training epoch {epoch}\")\n",
        "          data = data.to(device)\n",
        "          label = label.to(device)\n",
        "          \n",
        "          output = model(data)\n",
        "          loss = criterion(output, label)\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          acc = ((output.argmax(dim=1) == label).float().mean())\n",
        "          epoch_accuracy += acc/len(train_loader)\n",
        "          epoch_loss += loss/len(train_loader)\n",
        "          tepoch.set_postfix(loss=loss.item(), accuracy=100. * epoch_accuracy)\n",
        "      print('Epoch : {}, train accuracy : {}, train loss : {}'.format(epoch, epoch_accuracy,epoch_loss))\n",
        "\n",
        "    \n",
        "      with torch.no_grad():\n",
        "          epoch_val_accuracy=0\n",
        "          epoch_val_loss =0\n",
        "          with tqdm(val_loader, unit=\"iteration\") as tqdm_wrapped_valid_loader:\n",
        "            tepoch.set_description(f\"Validation after epoch {epoch}\")\n",
        "            for data, label in tqdm_wrapped_valid_loader:\n",
        "                data = data.to(device)\n",
        "                label = label.to(device)\n",
        "                \n",
        "                val_output = model(data)\n",
        "                val_loss = criterion(val_output,label)\n",
        "                \n",
        "                \n",
        "                acc = ((val_output.argmax(dim=1) == label).float().mean())\n",
        "                epoch_val_accuracy += acc/ len(val_loader)\n",
        "                epoch_val_loss += val_loss/ len(val_loader)\n",
        "                tqdm_wrapped_valid_loader.set_postfix(epoch_val_accuracy=epoch_val_accuracy, val_loss=val_loss)\n",
        "              \n",
        "          print('Epoch : {}, val_accuracy : {}, val_loss : {}'.format(epoch, epoch_val_accuracy,epoch_val_loss))\n",
        "    writer.add_scalar(\"train_accuracy\", epoch_accuracy, epoch)\n",
        "    writer.add_scalar(\"val_accuracy\", epoch_val_accuracy, epoch)\n",
        "    writer.add_scalar(\"train_loss_avg_iteration\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"val_loss_avg_iteration\", epoch_val_loss, epoch)\n",
        "    scalar_val_loss = epoch_val_loss.item()\n",
        "    if scalar_val_loss < min_val_loss:\n",
        "      min_val_loss = scalar_val_loss \n",
        "      epochs_since_min_loss = 0 \n",
        "      print(\"New best model, min_val_loss:\", min_val_loss)\n",
        "    else:\n",
        "      epochs_since_min_loss+=1\n",
        "      print(\"epochs_since_min_loss\",epochs_since_min_loss)\n",
        "    if epochs_since_min_loss > patience:\n",
        "      print(\"Early stopping.\")\n",
        "      break\n",
        "    \n",
        "\n",
        "writer.flush()\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "UvkN7MXIEjTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jp_c7__a78GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dog_probs = []\n",
        "model.eval()\n",
        "i=0\n",
        "with torch.no_grad():\n",
        "    for data, fileid in val_loader:\n",
        "        i+=1\n",
        "        if i>10:\n",
        "          break\n",
        "        data = data.to(device)\n",
        "        preds = model(data)\n",
        "        print(preds)\n",
        "        preds_list = F.softmax(preds, dim=1)[:, 1].tolist() #https://stats.stackexchange.com/questions/542054/why-does-torchvision-models-resnet18-not-use-softmax\n",
        "        dog_probs += list(zip(list(fileid), preds_list))"
      ],
      "metadata": {
        "id": "xGb3zoqwEmWl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}